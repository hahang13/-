import csv
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize
from collections import Counter
from nltk.corpus import stopwords

nltk.download('vader_lexicon')
nltk.download('stopwords')

stopwords = '``','…'
stop_iist = stopwords.split(',')

# CSV 파일 열기
with open('../뉴스 크롤링/관련주 크롤링/하이닉스1.csv', 'r', encoding='utf-8') as file:
    reader = csv.reader(file)
    
    # 헤더 읽기
    header = next(reader)
    
    # 텍스트 데이터 열 인덱스 지정
    text_column_index = 1  # 예시로 두 번째 열을 텍스트 데이터로 가정
    
    # 텍스트 데이터 토큰화 및 감정 분석
    tokens = []
    for row in reader:
        text = row[text_column_index]
        text_tokens = word_tokenize(text)  # 단어 토큰화
        tokens.extend(text_tokens)
        
    # 토큰화된 단어들의 빈도수 계산
    word_freq = Counter(tokens)
    
    # 감정 분석 수행
    sia = SentimentIntensityAnalyzer()
    sentiment_scores = {}
    for word, freq in word_freq.items():
        sentiment_score = sia.polarity_scores(word)['compound']
        sentiment_scores[word] = sentiment_score * freq
        
# 감정 분석 결과 출력
for word, score in sentiment_scores.items():
    print(f"{word}: {score}")
